import os
import gradio as gr
import tempfile
from humanomni import model_init, mm_infer
from humanomni.utils import disable_torch_init
from transformers import BertTokenizer
from chat_module.get_response import get_openai_response

class EmotionAnalyzer:
    def __init__(self):
        self.model = None
        self.processor = None
        self.tokenizer = None
        self.bert_tokenizer = None
        self.model_loaded = False
        
    def load_model(self):
        if not self.model_loaded:
            bert_model = "/home/zhzhu/model/bert-base-uncased"
            self.bert_tokenizer = BertTokenizer.from_pretrained(bert_model)
            disable_torch_init()
            model_path = "/home/zhzhu/model/R1-Omni-0.5B"
            self.model, self.processor, self.tokenizer = model_init(model_path)
            self.model_loaded = True
    
    def analyze_emotion(self, video_file, modal_type):
        try:
            if not self.model_loaded:
                self.load_model()
            
            instruct = "As an emotional recognition expert; throughout the video, which emotion conveyed by the characters is the most obvious to you? Output the thinking process in <think> </think> and final emotion in <answer> </answer> tags."
            
            video_tensor = self.processor['video'](video_file)
            
            audio_text = ""
            if modal_type == 'video_audio' or modal_type == 'audio':
                audio = self.processor['audio'](video_file)[0]
                if hasattr(self.model.get_audio_tower(), 'transcribe_audio'):
                    audio_text = self.model.get_audio_tower().transcribe_audio(audio)
            else:
                audio = None
            
            output = mm_infer(
                video_tensor, 
                instruct, 
                model=self.model, 
                tokenizer=self.tokenizer, 
                modal=modal_type, 
                question=instruct, 
                bert_tokeni=self.bert_tokenizer, 
                do_sample=False, 
                audio=audio
            )
            
            return output, audio_text
            
        except Exception as e:
            return f"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}", ""

analyzer = EmotionAnalyzer()

def analyze_video(video_file, modal_type):
    if video_file is None:
        return "è¯·ä¸Šä¼ è§†é¢‘æ–‡ä»¶", ""
    
    emotion_result, audio_text = analyzer.analyze_emotion(video_file, modal_type)
    return emotion_result, audio_text

def chat_with_gpt(emotion_result, audio_text):
    if not emotion_result or emotion_result == "è¯·ä¸Šä¼ è§†é¢‘æ–‡ä»¶":
        return "è¯·å…ˆè¿›è¡Œæƒ…æ„Ÿåˆ†æ"
    
    if not audio_text.strip():
        return "æ²¡æœ‰æ£€æµ‹åˆ°éŸ³é¢‘è½¬å½•æ–‡æœ¬ï¼Œè¯·ç¡®ä¿è§†é¢‘åŒ…å«éŸ³é¢‘å†…å®¹"
    
    try:
        response = get_openai_response(emotion_result, audio_text)
        return response
    except Exception as e:
        return f"å¯¹è¯ç”Ÿæˆå¤±è´¥: {str(e)}"

# åˆ›å»ºGradioç•Œé¢
with gr.Blocks(title="è§†é¢‘æƒ…æ„Ÿåˆ†æ Demo") as demo:
    gr.Markdown("# ğŸ­ è§†é¢‘æƒ…æ„Ÿåˆ†æä¸æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ")
    gr.Markdown("ä¸Šä¼ è§†é¢‘æ–‡ä»¶ï¼Œç³»ç»Ÿå°†åˆ†æè§†é¢‘ä¸­äººç‰©çš„æƒ…æ„Ÿè¡¨è¾¾ï¼Œå¹¶å¯ä»¥åŸºäºåˆ†æç»“æœè¿›è¡Œå¯¹è¯")
    
    with gr.Row():
        with gr.Column(scale=1):
            video_input = gr.Video(
                label="ä¸Šä¼ è§†é¢‘æ–‡ä»¶",
                format="mp4"
            )
            
            modal_select = gr.Radio(
                choices=["video", "video_audio", "audio"],
                value="video_audio",
                label="åˆ†ææ¨¡æ€",
                info="é€‰æ‹©è¦åˆ†æçš„æ¨¡æ€ç±»å‹"
            )
            
            analyze_btn = gr.Button("å¼€å§‹åˆ†æ", variant="primary")
            
        with gr.Column(scale=1):
            audio_text_output = gr.Textbox(
                label="éŸ³é¢‘è½¬å½•æ–‡æœ¬",
                lines=5,
                max_lines=8,
                placeholder="å½“é€‰æ‹©åŒ…å«éŸ³é¢‘çš„æ¨¡æ€æ—¶ï¼Œè¿™é‡Œå°†æ˜¾ç¤ºéŸ³é¢‘è½¬å½•çš„æ–‡æœ¬å†…å®¹"
            )
            
            result_output = gr.Textbox(
                label="æƒ…æ„Ÿåˆ†æç»“æœ",
                lines=12,
                max_lines=15
            )
    
    # æ™ºèƒ½å¯¹è¯åŠŸèƒ½
    gr.Markdown("## ğŸ’¬ æ™ºèƒ½å¯¹è¯")
    with gr.Row():
        with gr.Column(scale=1):
            chat_btn = gr.Button("åŸºäºéŸ³é¢‘æ–‡æœ¬ç”Ÿæˆå¯¹è¯", variant="secondary")
        
        with gr.Column(scale=2):
            chat_output = gr.Textbox(
                label="AIå›å¤",
                lines=8,
                max_lines=12,
                placeholder="AIå°†åŸºäºæƒ…æ„Ÿåˆ†æç»“æœå’ŒéŸ³é¢‘è½¬å½•æ–‡æœ¬ç»™å‡ºç›¸åº”å›å¤"
            )
    
    gr.Markdown("## ğŸ“ ä½¿ç”¨è¯´æ˜")
    gr.Markdown("""
    1. **ä¸Šä¼ è§†é¢‘**: ç‚¹å‡»è§†é¢‘ä¸Šä¼ åŒºåŸŸï¼Œé€‰æ‹©è¦åˆ†æçš„è§†é¢‘æ–‡ä»¶
    2. **é€‰æ‹©æ¨¡æ€**: 
       - `video`: ä»…åˆ†æè§†é¢‘ç”»é¢
       - `video_audio`: åŒæ—¶åˆ†æè§†é¢‘å’ŒéŸ³é¢‘ï¼ˆæ¨èï¼‰
       - `audio`: ä»…åˆ†æéŸ³é¢‘
    3. **å¼€å§‹åˆ†æ**: ç‚¹å‡»åˆ†ææŒ‰é’®è·å–ç»“æœ
    4. **æ™ºèƒ½å¯¹è¯**: åœ¨æƒ…æ„Ÿåˆ†æå®Œæˆåï¼Œç‚¹å‡»å¯¹è¯æŒ‰é’®åŸºäºéŸ³é¢‘è½¬å½•æ–‡æœ¬ç”ŸæˆAIå›å¤
    
    **æ³¨æ„**: 
    - é¦–æ¬¡ä½¿ç”¨æ—¶æ¨¡å‹åŠ è½½å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…
    - å¯¹è¯åŠŸèƒ½éœ€è¦å…ˆå®Œæˆæƒ…æ„Ÿåˆ†æä¸”éŸ³é¢‘è½¬å½•æœ‰å†…å®¹
    """)
    
    analyze_btn.click(
        fn=analyze_video,
        inputs=[video_input, modal_select],
        outputs=[result_output, audio_text_output]
    )
    
    chat_btn.click(
        fn=chat_with_gpt,
        inputs=[result_output, audio_text_output],
        outputs=[chat_output]
    )

if __name__ == "__main__":
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        debug=True
    )