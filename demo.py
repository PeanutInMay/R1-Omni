import os
import gradio as gr
import tempfile
from humanomni import model_init, mm_infer
from humanomni.utils import disable_torch_init
from transformers import BertTokenizer

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['TRANSFORMERS_OFFLINE'] = '1'
os.environ['CUDA_VISIBLE_DEVICES'] = '1'

class EmotionAnalyzer:
    def __init__(self):
        self.model = None
        self.processor = None
        self.tokenizer = None
        self.bert_tokenizer = None
        self.model_loaded = False
        
    def load_model(self):
        if not self.model_loaded:
            # åˆå§‹åŒ–BERTåˆ†è¯å™¨
            bert_model = "/home/zhzhu/model/bert-base-uncased"
            self.bert_tokenizer = BertTokenizer.from_pretrained(bert_model)
            
            # ç¦ç”¨Torchåˆå§‹åŒ–
            disable_torch_init()
            
            # åˆå§‹åŒ–æ¨¡å‹ã€å¤„ç†å™¨å’Œåˆ†è¯å™¨
            model_path = "/home/zhzhu/model/R1-Omni-0.5B"
            self.model, self.processor, self.tokenizer = model_init(model_path)
            self.model_loaded = True
    
    def analyze_emotion(self, video_file, modal_type):
        try:
            # ç¡®ä¿æ¨¡å‹å·²åŠ è½½
            if not self.model_loaded:
                self.load_model()
            
            # ä½¿ç”¨é»˜è®¤æŒ‡ä»¤
            instruct = "As an emotional recognition expert; throughout the video, which emotion conveyed by the characters is the most obvious to you? Output the thinking process in <think> </think> and final emotion in <answer> </answer> tags."
            
            # å¤„ç†è§†é¢‘è¾“å…¥
            video_tensor = self.processor['video'](video_file)
            
            # æ ¹æ®modalç±»å‹å†³å®šæ˜¯å¦å¤„ç†éŸ³é¢‘
            audio_text = ""
            if modal_type == 'video_audio' or modal_type == 'audio':
                audio = self.processor['audio'](video_file)[0]
                
                # è·å–éŸ³é¢‘è½¬å½•æ–‡æœ¬
                if hasattr(self.model.get_audio_tower(), 'transcribe_audio'):
                    audio_text = self.model.get_audio_tower().transcribe_audio(audio)
            else:
                audio = None
            
            # æ‰§è¡Œæ¨ç†
            output = mm_infer(
                video_tensor, 
                instruct, 
                model=self.model, 
                tokenizer=self.tokenizer, 
                modal=modal_type, 
                question=instruct, 
                bert_tokeni=self.bert_tokenizer, 
                do_sample=False, 
                audio=audio
            )
            
            return output, audio_text
            
        except Exception as e:
            return f"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}", ""

# åˆ›å»ºåˆ†æå™¨å®ä¾‹
analyzer = EmotionAnalyzer()

def analyze_video(video_file, modal_type):
    if video_file is None:
        return "è¯·ä¸Šä¼ è§†é¢‘æ–‡ä»¶", ""
    
    emotion_result, audio_text = analyzer.analyze_emotion(video_file, modal_type)
    return emotion_result, audio_text

# åˆ›å»ºGradioç•Œé¢
with gr.Blocks(title="è§†é¢‘æƒ…æ„Ÿåˆ†æ Demo") as demo:
    gr.Markdown("# ğŸ­ è§†é¢‘æƒ…æ„Ÿåˆ†æç³»ç»Ÿ")
    gr.Markdown("ä¸Šä¼ è§†é¢‘æ–‡ä»¶ï¼Œç³»ç»Ÿå°†åˆ†æè§†é¢‘ä¸­äººç‰©çš„æƒ…æ„Ÿè¡¨è¾¾")
    
    with gr.Row():
        with gr.Column(scale=1):
            # è§†é¢‘ä¸Šä¼ ç»„ä»¶
            video_input = gr.Video(
                label="ä¸Šä¼ è§†é¢‘æ–‡ä»¶",
                format="mp4"
            )
            
            # æ¨¡æ€é€‰æ‹©
            modal_select = gr.Radio(
                choices=["video", "video_audio", "audio"],
                value="video_audio",
                label="åˆ†ææ¨¡æ€",
                info="é€‰æ‹©è¦åˆ†æçš„æ¨¡æ€ç±»å‹"
            )
            
            # åˆ†ææŒ‰é’®
            analyze_btn = gr.Button("å¼€å§‹åˆ†æ", variant="primary")
            
        with gr.Column(scale=1):
            # éŸ³é¢‘è½¬å½•ç»“æœæ˜¾ç¤º
            audio_text_output = gr.Textbox(
                label="éŸ³é¢‘è½¬å½•æ–‡æœ¬",
                lines=5,
                max_lines=8,
                placeholder="å½“é€‰æ‹©åŒ…å«éŸ³é¢‘çš„æ¨¡æ€æ—¶ï¼Œè¿™é‡Œå°†æ˜¾ç¤ºéŸ³é¢‘è½¬å½•çš„æ–‡æœ¬å†…å®¹"
            )
            
            # æƒ…æ„Ÿåˆ†æç»“æœæ˜¾ç¤º
            result_output = gr.Textbox(
                label="æƒ…æ„Ÿåˆ†æç»“æœ",
                lines=12,
                max_lines=15
            )
    
    # ç¤ºä¾‹åŒºåŸŸ
    gr.Markdown("## ğŸ“ ä½¿ç”¨è¯´æ˜")
    gr.Markdown("""
    1. **ä¸Šä¼ è§†é¢‘**: ç‚¹å‡»è§†é¢‘ä¸Šä¼ åŒºåŸŸï¼Œé€‰æ‹©è¦åˆ†æçš„è§†é¢‘æ–‡ä»¶
    2. **é€‰æ‹©æ¨¡æ€**: 
       - `video`: ä»…åˆ†æè§†é¢‘ç”»é¢
       - `video_audio`: åŒæ—¶åˆ†æè§†é¢‘å’ŒéŸ³é¢‘ï¼ˆæ¨èï¼‰
       - `audio`: ä»…åˆ†æéŸ³é¢‘
    3. **å¼€å§‹åˆ†æ**: ç‚¹å‡»åˆ†ææŒ‰é’®è·å–ç»“æœ
    
    **æ³¨æ„**: 
    - é¦–æ¬¡ä½¿ç”¨æ—¶æ¨¡å‹åŠ è½½å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…
    - å½“é€‰æ‹©åŒ…å«éŸ³é¢‘çš„æ¨¡æ€æ—¶ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨è½¬å½•éŸ³é¢‘å†…å®¹å¹¶æ˜¾ç¤ºåœ¨ä¸Šæ–¹æ–‡æœ¬æ¡†ä¸­
    """)
    
    analyze_btn.click(
        fn=analyze_video,
        inputs=[video_input, modal_select],
        outputs=[result_output, audio_text_output]
    )

if __name__ == "__main__":
    demo.launch(
        server_name="0.0.0.0",  # å…è®¸å¤–éƒ¨è®¿é—®
        server_port=7860,       # ç«¯å£å·
        share=False,            # è®¾ç½®ä¸ºTrueå¯ä»¥ç”Ÿæˆå…¬å¼€é“¾æ¥
        debug=True              # è°ƒè¯•æ¨¡å¼
    )